GPU Driver '470' detected
WARNING: You are using pip version 20.1.1; however, version 23.1.2 is available.
You should consider upgrading via the '/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/bin/python -m pip install --upgrade pip' command.
/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run.py", line 137, in <module>
    evaluate(args, save_results=True)
  File "run.py", line 108, in evaluate
    result_matrix = evaluate_all_models_over_all_domains(args, cl_run_input, save_results=save_results)
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/cl_domain/evaluation.py", line 31, in evaluate_all_models_over_all_domains
    result_matrix[i, j] = Trainer(
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3029, in evaluate
    output = eval_loop(
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3318, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/cl_domain/evaluation.py", line 65, in compute_metrics
    p = Path(f"{args['cl_predictions_dir']}/{args['cl_super_run_label']}/{args['cl_run_label']}")
KeyError: 'cl_run_label'
100%|██████████| 1/1 [00:12<00:00, 12.23s/it]/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run.py", line 137, in <module>
    evaluate(args, save_results=True)
  File "run.py", line 108, in evaluate
    result_matrix = evaluate_all_models_over_all_domains(args, cl_run_input, save_results=save_results)
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/cl_domain/evaluation.py", line 31, in evaluate_all_models_over_all_domains
    result_matrix[i, j] = Trainer(
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3029, in evaluate
    output = eval_loop(
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/venv/lib/python3.8/site-packages/transformers/trainer.py", line 3318, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File "/blue/boyer/amogh.mannekote/continual-learning-nlu/continual-learning-nlu/cl_domain/evaluation.py", line 65, in compute_metrics
    p = Path(f"{args['cl_predictions_dir']}/{args['cl_super_run_label']}/{args['cl_run_label']}")
KeyError: 'cl_run_label'
100%|██████████| 1/1 [00:12<00:00, 12.21s/it]